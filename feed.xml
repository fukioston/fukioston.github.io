<feed xmlns="http://www.w3.org/2005/Atom"> <id>https://fukioston.github.io/</id><title>Fukioston</title><subtitle>我好像不是很不厉害。</subtitle> <updated>2025-12-13T23:12:23+08:00</updated> <author> <name>fukioston</name> <uri>https://fukioston.github.io/</uri> </author><link rel="self" type="application/atom+xml" href="https://fukioston.github.io/feed.xml"/><link rel="alternate" type="text/html" hreflang="en" href="https://fukioston.github.io/"/> <generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator> <rights> © 2025 fukioston </rights> <icon>/assets/img/favicons/favicon.ico</icon> <logo>/assets/img/favicons/favicon-96x96.png</logo> <entry><title>Transformer相关代码</title><link href="https://fukioston.github.io/posts/transformer_code/" rel="alternate" type="text/html" title="Transformer相关代码" /><published>2025-12-11T11:33:00+08:00</published> <updated>2025-12-13T23:11:57+08:00</updated> <id>https://fukioston.github.io/posts/transformer_code/</id> <content src="https://fukioston.github.io/posts/transformer_code/" /> <author> <name>Fukioston</name> </author> <category term="LLM" /> <category term="学习笔记" /> <summary> self-attention代码 from math import sqrt import torch import torch.nn as nn class SelfAttention(nn.Module): def __init__(self, dim_embedding, dim_qk, dim_v): super(SelfAttention, self).__init__() self.dim_embedding = dim_embedding self.dim_qk = dim_qk self.dim_v = dim_v # 定义线性变换函数 self.linear_q = nn.Linear(dim_embedding, dim_qk, bias=False) ... </summary> </entry> <entry><title>Transformer学习笔记</title><link href="https://fukioston.github.io/posts/transformer/" rel="alternate" type="text/html" title="Transformer学习笔记" /><published>2025-12-11T11:33:00+08:00</published> <updated>2025-12-11T11:33:00+08:00</updated> <id>https://fukioston.github.io/posts/transformer/</id> <content src="https://fukioston.github.io/posts/transformer/" /> <author> <name>Fukioston</name> </author> <category term="LLM" /> <category term="学习笔记" /> <summary> 此文为本人学习b站up主RethinkFun的视频的笔记，表达上不具科普效果，有点抱歉 Tokenize 即分词，把一句话分成若干个词。 这些可学习参数就是Embedding Embedding Embedding的每一个维度可以认为代表不同的语义，可以有很多维度。 Self-attention 主要是Q,K,V Q表示的是向其他token查询的向量 K表示应答其他token查询的向量 V表示的是更新其他token的embedding的向量 Q和K先匹配，获得相似度，然后进行softmax，这样就先算出该单词和其他单词的相似度，接着根据这个相似度就可以重新使用大家的V来进行该单词的重新表达 可以类比为查字典的过程，K是字典里面的关键词，Q则是查询，V是字典里面关键词对应的内容。 简单说：Q 和 K 负责 “选哪些词重要”，V 负责 “把重要的词的信息整合起来”。... </summary> </entry> </feed>