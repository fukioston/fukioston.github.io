---
title: Transformer学习笔记
author: Fukioston
date: 2025-12-01 11:33:00 +0800
categories: [LLM,学习笔记]
tags: [LLM,学习笔记]
pin: false
math: true
mermaid: true
image:
  path: /../assets/img/te/img.png
  lqip: false
  alt: Responsive rendering of Chirpy theme on multiple devices.
---
<small>此文为本人学习b站up主RethinkFun的视频的笔记，表达上不具科普效果，有点抱歉</small>

# Tokenize
即分词，把一句话分成若干个词。
![alt text](/assets/img/transformer/tokenize.png)

这些可学习参数就是Embedding

# Embedding
Embedding的每一个维度可以认为代表不同的语义，可以有很多维度。
![alt text](/assets/img/transformer/embedding.png)

# Self-attention
主要是Q,K,V
Q表示的是向其他token查询的向量

K表示应答其他token查询的向量

V表示的是更新其他token的embedding的向量

Q和K先匹配，获得相似度，然后进行softmax，这样就先算出该单词和其他单词的相似度，接着根据这个相似度就可以重新使用大家的V来进行该单词的重新表达

![alt text](/assets/img/transformer/qkv.png)

# MultiHead-attention
其实就是多个自注意力机制，8组qkv获得8组v向量后拼接起来，于是得到了下面这个公式

![alt text](/assets/img/transformer/matten.png)

Q乘以K的转置就算出了刚才所说的相似度，用这个相似度进行softmax，再乘上V进行attention的表达